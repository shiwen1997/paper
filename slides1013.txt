1.同学们，大家好，我叫施文，这篇论文是2019年发表在NDSS会议上的一篇论文，论文的标题是：机器学习泄露：模型和数据独立的成员推理攻击和机器学习模型的防御。

这篇论文研究的领域是机器学习安全。

2.我将从下面五个方面介绍这篇论文。
第一个是背景，第二个是研究现状，第三个是本篇论文所作出的贡献，第四个是文中提到的攻击方法，第五个是文中提到关于所提出的攻击方法的防御机制。

3.首先说一下机器学习安全背景：
1）现在机器学习有很多是在包含隐私数据集上训练的，
例如公司的财政数据、用户的位置和行为数据、用户的生物医疗数据，有些共需要机器学习来预测公司财政的趋势。公司的财政数据被泄露给同行会导致公司亏损。用户的位置和行为数据，例如你在淘宝上搜牙刷等，淘宝会根据你的浏览记录利用机器学习算法给你推荐牙膏杯子等，但是当你的浏览数据被泄露会导致个人隐私泄露了。生物医疗数据例如病人的体检结果历史数据，被医院收集起来利用机器学习算法来训练一个身体症状跟可能得病的种类的机器学习模型。这些数据如果被泄露了就会导致很严重的后果。
2）ML模型很容易被攻击
模型逆向攻击：构造模型
模型提取攻击：提取模型数据、参数
对抗样本攻击：GAN对抗样本

4.介绍一下本文的背景，
本文所示用的MIA的应用背景
一个敌手想要确定一个数据项是否在一个ML模型的训练集中。
成功的MIA能造成严重的后果。

例如，你知道一个用户的医疗记录被用来训练某种医疗疾病的机器学习模型，那么就可以推断这个用户患有这种病。
（用户体检记录跟患有哪类疾病。）


5发展现状。
2017年Shokri第一次提出了MIA这个攻击来对抗ML模型。
这张图介绍了MIA的目的。















